# NetworkX Developer Discussions: June 16th, 2022

Previous meeting notes available [here](https://github.com/networkx/archive/tree/main/meetings). Please feel free to add topics for discussion and news items below.

**In attendance:** Matt, Ross, Mridul, Juanita, Konstantinos, Dilara, Kelly, Dan

## Welcome and news



## Important dates/deadlines

- NX 2.8.4 (6/11/22)
- NX 3 removal sprint (6/15/22 9-11am PDT...earlier for headstart with Ross)
- NX 3b1 (6/20/22?)
- NX 3rc1 (6/27/22?)
- NX 3 (7/15/22)
- Post NX 3 release: A roadmap review

- Euroscipy (Basel Switzerland) - NX sprint application is in, end of August

## Topics

- Should we keep a 2.8 available?
  * E.g. some libraries that can't immediately update
  * For example, still releasing 1.X series after 2.0 came out (for a few months)
  * Is sparse arrays the only thing that requires the scipy 1.8
  * Strong support for backporting
  * Discuss further next week

- Deprecation sprint follow-up: thoughts/feedback?
  * Note about reviewing:
    - I've been using `[skip ci]` in some commit messages when I e.g. push up a doc change. This has the unfortunate side effect of taking the green check mark away on the PR **summary** list. In cases like this, please refer to the last commit where CI was run.
    - MacOS pygraphviz errors in recent CI runs (intermittent?)

- pysal and NX 3.0: still using scipy sparse matrices
  * Other libraries that we can test against? In principle *they* should be testing against *us*.
  * PR open against PySAL ([link](https://github.com/pysal/libpysal/pull/472)) to update (thanks Matt!). Merged today.
  * Gallery examples will still fail until next `pysal` release - maybe holdoff merging the sparse conversion function removal or investigate other options

- What to do about `Filter*.copy` deprecations?
  * Warnings weren't being raised due to `stacklevel`.
    - Safe to remove anyways: major version bump, it's in the release notes, won't silently fail, hard to hit

- [name=Mridul Seth] Office Hours/Pair programming sessions for new contributors?
    - https://github.com/networkx/networkx/pull/5733 is an example PR where we can use something like a pair programming session as it would probably be much more quick to get them setup in a zoom call.
    - Great idea! General support - details TBD; start with responding to this particular user in the PR
   
- [name=rossbar] "Optimizing" GitHub actions workflows
  * In a previous meeting, we discussed the possibility of adding dependencies between jobs so that when one fails it automatically stops the others
    - This happens by default for jobs *within a testing matrix*
    - Goal: expand this behavior to the entire workflow.
  * Unfortunately, there doesn't seem to be an easy way to do this. Some ideas:
    1. Add inter-job dependencies [with `needs`](https://docs.github.com/en/actions/using-workflows/workflow-syntax-for-github-actions#jobsjob_idneeds). For example, make testing jobs *depend* on linting, which tends to be the fastest job.
       * Pros:
         - simple syntax, easy to understand workflow file
       * Cons:
         - Adds a sequential step: i.e. dependent jobs don't start until dependencies finish (Mridul: Linting is usually pretty quick)
         - Would elevate importance of linting job, i.e. testing doesn't run unless linting passes - do we care about linting that much? (Mridul: Probabaly not, but 4 times out of 5 linting will fail! so people would have to run the whole CI suite 2 times anyway.)
    2. Hack the `matrix` strategy in the workflow to encompass all jobs, which will automatically cancel concurrent jobs if any other fails: [see this suggested recipe](https://github.community/t/how-to-cancel-remaining-running-jobs-if-one-of-the-parallel-jobs-failed/185458/5)
       * This gives the behavior we want, but likely makes the workflow file harder to understand (more `if:`s per job)
    3. Add a post-job checker that will cancel the entire workflow if any job fails: [see this suggested recipe](https://github.community/t/how-to-cancel-remaining-running-jobs-if-one-of-the-parallel-jobs-failed/185458/4)
       * Flexible solution as we get to pick which jobs to post-validate
       * Adds cruft to workflow file
    4. Do nothing and wait until GitHub fixes this upstream :)
    5. Other ideas?
    
- Making the test suite more comprehensive (not coverage):
  * Check out `hypothesis` for setting up property-based testing
  * Developing good property-based tests:
    - Reference "naive" (but correct) implementation
    - Generate random graphs, evaluate them for the property you want
    - Pump these through the naive implementation and collect interesting cases for test suite

- Benchmarking!
  * Let's look at this more, starting with Mridul's PR

- Heads up: the 2.8.4 release shows significantly slower benchmarks - let's take a closer look: https://mriduls.com/nx-asv-benchmarks/

## Long-term tracking items

- Looking at the Roadmap - what do we want on there?
  * Some ideas:
    - GraphBLAS in the performance section
    - Temporal networks, hypergraphs, other new graph types?
    - Make testing more comprehensive -- mixed node types, hashable nodes, etc. (hypothesis?)
    - Benchmarking suites to at least run locally some timing.

- Temporal networks: how to think about these
  * seems to be a lot of interest from different communities, but no standard approach yet...
  * Having a use-case to drive the design of a data structure would be good
  
- Stop building/shipping pdf version of documentation?
  * This is being considered by both scipy and numpy and seems to have garnered mostly positive feedback
    - related scipy issue: https://github.com/scipy/scipy/issues/15635
  * revisit after 2.8 release
  * Keep up-to-date with feedback from other projects that are considering this

- nx-guides
  * We need a contributor guide :book: [name=Mridul]
  * Dependency management for tutorials - how to strike a balance between tools people use and maintainability

- New contributors
  * So many good students/contributions - how do we keep the ball rolling (even if we can only accept 1 or a subset of them for this round)
    - Add something to contributor guide about where to look for good issues (e.g. add missing docstring examples)
    - Link to [first issue tag](https://github.com/networkx/networkx/labels/Good%20First%20Issue) on github?
    
- Revisiting our pytest `slow` mark:
  * The `slow` tests are run w/ each push/PR in the coverage job; however, it's not necessarily true that the slow tests improve coverage!
  * Could save a lot of CI time by having a separate category (e.g. `pytest.mark.comprehensive`?) that is for tests that are slow but *don't* affect coverage. These could be run less frequently (scheduled job?) and potentially save several minutes for each push
    - General consensus: seems like a good idea
    - (Jarrod) maybe worth waiting until after the code removal sprint?

- Other CI-saving ideas
    - All tests should depend on the lint action, so all the other tests fail fast.
    - Another one: configure CI to stop running tests from a previous commit when a new commit is pushed up (I think scipy does this, so can copy their config)

## Discussion

- [ ] Social media
    * NetworkX twitter
    * blog.scientific-python.org

TODO: Fix Grey/Green checkmark problem on GH!
TODO: Update zoom link
  - Want everyone to be able to share screen by default
  - Move away from berkeley-link